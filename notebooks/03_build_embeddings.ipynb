{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b8ad52a",
   "metadata": {},
   "source": [
    "# **Build Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "496aeb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, spacy, re, unicodedata, ftfy, pathlib, gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511d6dfc",
   "metadata": {},
   "source": [
    "## **NLP** \n",
    "### _(Normalization + Tokenization + Stopword removal + Stemming)_\n",
    "\n",
    "- **Using spaCy instead of re.findall.**\n",
    "\n",
    "Regex is fast & simple, but it can split improperly on apostrophes/hyphens and misses non-ASCII tokens. spaCy’s tokenizer is more robust for messy, user-written queries.\n",
    "\n",
    "- **Using token.is_stop from spaCy instead nltk.corpus.stopwords**\n",
    "\n",
    "nltk.corpus.stopwords (≈ 180 words). While, token.is_stop from spaCy (≈ 500, incl. pronouns, auxiliaries).\tBigger list removes more “noise” words like ‘will’, ‘have’, ‘ourselves’ that rarely help similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3559af98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents processed: 456\n"
     ]
    }
   ],
   "source": [
    "# Load the model and deactivate ner(entities recognition), parser(subject-verb-object), tagger (grammatical labels)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"]) # tagger activate or deactivated?\n",
    "\n",
    "def preprocess(text: str) -> list[str]:\n",
    "    text = ftfy.fix_text(text) # Repairs mis-encoded characters (“â€™” → “’”).\n",
    "    text = unicodedata.normalize(\"NFKC\", text) #Normalize unicode \n",
    "    text = re.sub(r\"[‐-–—]\", \"-\", text)          # weird - in only one -\n",
    "    doc  = nlp(text.lower()) # Tokenization process with Spacy\n",
    "    return [t.lemma_ for t in doc if t.is_alpha and not t.is_stop] # Keep clean lemmas only\n",
    "\n",
    "# Performing the model\n",
    "# Load FAQ data\n",
    "faq_df = pd.read_csv(\"../data/processed/faqs.csv\")\n",
    "faq_df[\"text\"] = faq_df[\"question\"].astype(str)\n",
    "\n",
    "# Load resource data\n",
    "res_path = \"../data/processed/student_resources_index.csv\"\n",
    "\n",
    "try:\n",
    "    res_df = pd.read_csv(res_path)\n",
    "    if all(col in res_df.columns for col in [\"title\", \"description\"]):\n",
    "        res_df[\"text\"] = res_df[\"title\"].astype(str) + \" \" + res_df[\"description\"].astype(str)\n",
    "    else:\n",
    "        print(\"Columns 'title' and 'description' not found in resources CSV.\")\n",
    "        res_df = pd.DataFrame(columns=[\"text\"])\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {res_path}\")\n",
    "    res_df = pd.DataFrame(columns=[\"text\"])\n",
    "\n",
    "\n",
    "# Combine both into one corpus\n",
    "combined_texts = pd.concat([faq_df[\"text\"], res_df[\"text\"]], ignore_index=True)\n",
    "\n",
    "# Preprocess each entry\n",
    "sentences = combined_texts.apply(preprocess).tolist()\n",
    "\n",
    "print(f\"Total documents processed: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd152d9",
   "metadata": {},
   "source": [
    "In te previous code block We used `spaCy` to normalize and tokenize the questions from the FAQ corpus and the resources:\n",
    "\n",
    "- Applied `ftfy` and `unicodedata` to fix encoding issues.\n",
    "- Normalize Unicode characters.\n",
    "- Tokenized each question using `spaCy`'s English model (`en_core_web_sm`).\n",
    "- Filtered out non-alphabetic tokens and common stopwords.\n",
    "- Extracted the lemma of each remaining token to reduce words to their base form (e.g., \"studying\" → \"study\").\n",
    "\n",
    "This resulted in a cleaned, tokenized version of each question, ready to be used for embedding with Word2Vec or GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2776da2",
   "metadata": {},
   "source": [
    "## **Word2Vec**\n",
    "### _Vectorization_\n",
    "\n",
    "Converts words into numerical vectors that a model can compare or classify.\n",
    "\n",
    "- \"osap\" y \"bursary\" → closed embeddings.\n",
    "- \"vmock\" y \"resume\" → closed.\n",
    "- \"fees\" y \"library\" → so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "872fe202",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(\n",
    "        sentences=sentences, # Tokens processed previously\n",
    "        vector_size=100,# 100 dimension each vector\n",
    "        window=5, #Context 5 words prev and post\n",
    "        min_count=1, # Include all words even it appears more than one\n",
    "        workers=4, sg=1, \n",
    "        epochs=50, seed=42)\n",
    "w2v.save(\"../models/embeddings/word2vec_faqs.bin\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14556de6",
   "metadata": {},
   "source": [
    "### **GloVe Embeddings**\n",
    "\n",
    "In addition to training our own Word2Vec model on the FAQ and resource corpus, we also load pre-trained **GloVe embeddings (Global Vectors for Word Representation)**.\n",
    "\n",
    "GloVe was trained on massive corpora like Wikipedia and Gigaword, and captures rich semantic relationships between words (e.g., \"resume\" and \"job\" are close; \"fee\" and \"tuition\" are related). \n",
    "\n",
    "By using GloVe:\n",
    "- We can represent **words that don't appear often (or at all)** in our training corpus.\n",
    "- We benefit from **external general knowledge**, which helps improve chatbot responses.\n",
    "- It allows us to **compare** performance between our custom Word2Vec and industry-standard embeddings.\n",
    "\n",
    "We use the 100-dimensional `glove.6B.100d.txt` file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d77d7",
   "metadata": {},
   "source": [
    "### Setting Glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efc6664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where GloVe will be stored\n",
    "glove_path = pathlib.Path(\"../models/embeddings/glove.6B.100d.txt\")\n",
    "\n",
    "# Download and unzip if not already present\n",
    "if not glove_path.exists():\n",
    "    import zipfile, requests\n",
    "    zip_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "    zip_path = glove_path.parent / \"glove.6B.zip\"\n",
    "\n",
    "    print(\"Downloading GloVe...\")\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(requests.get(zip_url).content)\n",
    "    print(\"Unzipping...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(glove_path.parent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96dbb526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "glove = gensim.models.KeyedVectors.load_word2vec_format(glove_path, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5db3904",
   "metadata": {},
   "source": [
    "## **Vectorize whole Courpus**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d2526b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get sentence vector using GloVe\n",
    "def sent_vector(sent, model, dim=100):\n",
    "    tokens = preprocess(sent)\n",
    "    vecs = [model[t] for t in tokens if t in model] # Get vector each word\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c50a6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing with GloVe: 100%|██████████| 456/456 [00:01<00:00, 245.95it/s]\n",
      "Vectorizing with Word2Vec: 100%|██████████| 456/456 [00:01<00:00, 251.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings saved.\n",
      "✅ Word2Vec embeddings saved.\n"
     ]
    }
   ],
   "source": [
    "# Vectorize all questions/resources with GloVe\n",
    "tqdm.pandas(desc=\"Vectorizing with GloVe\") # Progress bar\n",
    "vectors_glove = combined_texts.progress_apply(lambda s: sent_vector(s, glove, dim=100))\n",
    "\n",
    "# Vectorize all (FAQ + resources)\n",
    "w2v = Word2Vec.load(\"../models/embeddings/word2vec_faqs.bin\")\n",
    "tqdm.pandas(desc=\"Vectorizing with Word2Vec\")\n",
    "vectors_w2v = combined_texts.progress_apply(lambda s: sent_vector(s, w2v.wv, dim=100))\n",
    "\n",
    "# Save as pickle for future use\n",
    "pd.DataFrame({\"vec_glove\": vectors_glove}).to_pickle(\"../data/processed/glove_vectors.pkl\") #Saving vectors\n",
    "print(\"GloVe embeddings saved.\")\n",
    "\n",
    "pd.DataFrame({\"vec_w2v\": vectors_w2v}).to_pickle(\"../data/processed/word2vec_vectors.pkl\")\n",
    "print(\"✅ Word2Vec embeddings saved.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
