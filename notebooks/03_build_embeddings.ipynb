{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b8ad52a",
   "metadata": {},
   "source": [
    "# **Build Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "496aeb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paula\\github-classroom\\Self-Service_Portal_Final_Project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import pandas as pd, spacy, re, unicodedata, ftfy, pathlib, gensim\n",
    "from gensim.models import Word2Vec\n",
    "from embedding_hf import encode_texts\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511d6dfc",
   "metadata": {},
   "source": [
    "## **NLP** \n",
    "### _(Normalization + Tokenization + Stopword removal + Stemming)_\n",
    "\n",
    "- **Using spaCy instead of re.findall.**\n",
    "\n",
    "Regex is fast & simple, but it can split improperly on apostrophes/hyphens and misses non-ASCII tokens. spaCy‚Äôs tokenizer is more robust for messy, user-written queries.\n",
    "\n",
    "- **Using token.is_stop from spaCy instead nltk.corpus.stopwords**\n",
    "\n",
    "nltk.corpus.stopwords (‚âà 180 words). While, token.is_stop from spaCy (‚âà 500, incl. pronouns, auxiliaries).\tBigger list removes more ‚Äúnoise‚Äù words like ‚Äòwill‚Äô, ‚Äòhave‚Äô, ‚Äòourselves‚Äô that rarely help similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3559af98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents processed: 456\n"
     ]
    }
   ],
   "source": [
    "# Load the model and deactivate ner(entities recognition), parser(subject-verb-object), tagger (grammatical labels)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"]) # tagger activate or deactivated?\n",
    "\n",
    "def preprocess(text: str) -> list[str]:\n",
    "    text = ftfy.fix_text(text) # Repairs mis-encoded characters (‚Äú√¢‚Ç¨‚Ñ¢‚Äù ‚Üí ‚Äú‚Äô‚Äù).\n",
    "    text = unicodedata.normalize(\"NFKC\", text) #Normalize unicode \n",
    "    text = re.sub(r\"[‚Äê-‚Äì‚Äî]\", \"-\", text)          # weird - in only one -\n",
    "    doc  = nlp(text.lower()) # Tokenization process with Spacy\n",
    "    return [t.lemma_ for t in doc if t.is_alpha and not t.is_stop] # Keep clean lemmas only\n",
    "\n",
    "# Performing the model\n",
    "# Load FAQ data\n",
    "faq_df = pd.read_csv(\"../data/processed/faqs.csv\")\n",
    "faq_df[\"text\"] = faq_df[\"question\"].astype(str)\n",
    "\n",
    "# Load resource data\n",
    "res_path = \"../data/processed/student_resources_index.csv\"\n",
    "\n",
    "try:\n",
    "    res_df = pd.read_csv(res_path)\n",
    "    if all(col in res_df.columns for col in [\"title\", \"description\"]):\n",
    "        res_df[\"text\"] = res_df[\"title\"].astype(str) + \" \" + res_df[\"description\"].astype(str)\n",
    "    else:\n",
    "        print(\"Columns 'title' and 'description' not found in resources CSV.\")\n",
    "        res_df = pd.DataFrame(columns=[\"text\"])\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {res_path}\")\n",
    "    res_df = pd.DataFrame(columns=[\"text\"])\n",
    "\n",
    "\n",
    "# Combine both into one corpus\n",
    "combined_texts = pd.concat([faq_df[\"text\"], res_df[\"text\"]], ignore_index=True)\n",
    "\n",
    "# Preprocess each entry\n",
    "sentences = combined_texts.apply(preprocess).tolist()\n",
    "\n",
    "print(f\"Total documents processed: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd152d9",
   "metadata": {},
   "source": [
    "In te previous code block We used `spaCy` to normalize and tokenize the questions from the FAQ corpus and the resources:\n",
    "\n",
    "- Applied `ftfy` and `unicodedata` to fix encoding issues.\n",
    "- Normalize Unicode characters.\n",
    "- Tokenized each question using `spaCy`'s English model (`en_core_web_sm`).\n",
    "- Filtered out non-alphabetic tokens and common stopwords.\n",
    "- Extracted the lemma of each remaining token to reduce words to their base form (e.g., \"studying\" ‚Üí \"study\").\n",
    "\n",
    "This resulted in a cleaned, tokenized version of each question, ready to be used for embedding with Word2Vec or GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2776da2",
   "metadata": {},
   "source": [
    "## **Word2Vec**\n",
    "### _Vectorization_\n",
    "\n",
    "Converts words into numerical vectors that a model can compare or classify.\n",
    "\n",
    "- \"osap\" y \"bursary\" ‚Üí closed embeddings.\n",
    "- \"vmock\" y \"resume\" ‚Üí closed.\n",
    "- \"fees\" y \"library\" ‚Üí so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872fe202",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(\n",
    "        sentences=sentences, # Tokens processed previously\n",
    "        vector_size=100,# 100 dimension each vector\n",
    "        window=5, #Context 5 words prev and post\n",
    "        min_count=1, # Include all words even it appears more than one\n",
    "        workers=4, sg=1, \n",
    "        epochs=50, seed=42)\n",
    "w2v.save(\"../models/embeddings/word2vec_faqs.bin\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14556de6",
   "metadata": {},
   "source": [
    "### **GloVe Embeddings**\n",
    "\n",
    "In addition to training our own Word2Vec model on the FAQ and resource corpus, we also load pre-trained **GloVe embeddings (Global Vectors for Word Representation)**.\n",
    "\n",
    "GloVe was trained on massive corpora like Wikipedia and Gigaword, and captures rich semantic relationships between words (e.g., \"resume\" and \"job\" are close; \"fee\" and \"tuition\" are related). \n",
    "\n",
    "By using GloVe:\n",
    "- We can represent **words that don't appear often (or at all)** in our training corpus.\n",
    "- We benefit from **external general knowledge**, which helps improve chatbot responses.\n",
    "- It allows us to **compare** performance between our custom Word2Vec and industry-standard embeddings.\n",
    "\n",
    "We use the 100-dimensional `glove.6B.100d.txt` file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d77d7",
   "metadata": {},
   "source": [
    "### Setting Glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efc6664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where GloVe will be stored\n",
    "glove_path = pathlib.Path(\"../models/embeddings/glove.6B.100d.txt\")\n",
    "\n",
    "# Download and unzip if not already present\n",
    "if not glove_path.exists():\n",
    "    import zipfile, requests\n",
    "    zip_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "    zip_path = glove_path.parent / \"glove.6B.zip\"\n",
    "\n",
    "    print(\"Downloading GloVe...\")\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(requests.get(zip_url).content)\n",
    "    print(\"Unzipping...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(glove_path.parent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96dbb526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "glove = gensim.models.KeyedVectors.load_word2vec_format(glove_path, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5db3904",
   "metadata": {},
   "source": [
    "## **Vectorize whole Courpus**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d2526b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get sentence vector using GloVe\n",
    "def sent_vector(sent, model, dim=100):\n",
    "    tokens = preprocess(sent)\n",
    "    vecs = [model[t] for t in tokens if t in model] # Get vector each word\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c50a6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing with GloVe: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 456/456 [00:01<00:00, 238.98it/s]\n",
      "Vectorizing with Word2Vec: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 456/456 [00:01<00:00, 250.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings saved.\n",
      "‚úÖ Word2Vec embeddings saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectorize all questions/resources with GloVe\n",
    "tqdm.pandas(desc=\"Vectorizing with GloVe\") # Progress bar\n",
    "vectors_glove = combined_texts.progress_apply(lambda s: sent_vector(s, glove, dim=100))\n",
    "\n",
    "# Vectorize all (FAQ + resources)\n",
    "w2v = Word2Vec.load(\"../models/embeddings/word2vec_faqs.bin\")\n",
    "tqdm.pandas(desc=\"Vectorizing with Word2Vec\")\n",
    "vectors_w2v = combined_texts.progress_apply(lambda s: sent_vector(s, w2v.wv, dim=100))\n",
    "\n",
    "# Save as pickle for future use\n",
    "pd.DataFrame({\"vec_glove\": vectors_glove}).to_pickle(\"../data/processed/glove_vectors.pkl\") #Saving vectors\n",
    "print(\"GloVe embeddings saved.\")\n",
    "\n",
    "pd.DataFrame({\"vec_w2v\": vectors_w2v}).to_pickle(\"../data/processed/word2vec_vectors.pkl\")\n",
    "print(\"‚úÖ Word2Vec embeddings saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21caf3f",
   "metadata": {},
   "source": [
    "## **Hugging Face embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87cbed65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paula\\github-classroom\\Self-Service_Portal_Final_Project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Encoding with Hugging Face sentence-transformers...\n",
      "‚úÖ Hugging Face embeddings saved to: ../data/processed/hf_vectors.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Encoding with Hugging Face sentence-transformers...\")\n",
    "hf_vectors = encode_texts(combined_texts.tolist())  # Convert Series to list of strings\n",
    "\n",
    "# Save embeddings\n",
    "hf_path = \"../data/processed/hf_vectors.pkl\"\n",
    "with open(hf_path, \"wb\") as f:\n",
    "    pickle.dump(hf_vectors, f)\n",
    "\n",
    "print(f\"‚úÖ Hugging Face embeddings saved to: {hf_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
