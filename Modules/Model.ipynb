{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b8ad52a",
   "metadata": {},
   "source": [
    "# step 1 load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f38f8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 documents.\n"
     ]
    }
   ],
   "source": [
    "# Example: Load text files from a folder\n",
    "import os\n",
    "\n",
    "def load_documents(folder_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents\n",
    "\n",
    "# Replace 'sample_docs/' with your actual folder\n",
    "documents = load_documents('sschatbot_docs/')\n",
    "print(f\"Loaded {len(documents)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72213a4a",
   "metadata": {},
   "source": [
    "# Step 2: Tokenizer\n",
    "\n",
    "In this section we created a basic tokenizer to process the text documents. This tokenizer split each document into tokens (words) and removes punctuation. It also converts all tokens to lowercase and with a regular expression to remove any non-alphanumeric characters.\n",
    "\n",
    "At the end of this block, we will have a list of tokens for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f426785c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['academic', 'policies', 'and', 'procedures', 'faq', 'q', 'what', 'are', 'the', 'important', 'academic', 'policies', 'i', 'need', 'to', 'know', 'a', 'you', 'should', 'familiarize']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Test on one document\n",
    "tokens = tokenize(documents[0])\n",
    "print(tokens[:20])  # Preview first 20 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cac86f",
   "metadata": {},
   "source": [
    "# Step 3: Normalization Pipeline (Stemming, Stop Word Removal, etc.)\n",
    "Using nltk library, we will implement a normalization pipeline that includes stemming and stop word removal. This will help us reduce the vocabulary size and focus on the most relevant terms in our documents.\n",
    "\n",
    "For example, the word \"anyone\" will be stemmed to \"anyon\", and \"glimpse\" will be stemmed to \"glimps\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18aef344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['academ', 'polici', 'procedur', 'faq', 'q', 'import', 'academ', 'polici', 'need', 'know', 'familiar', 'academ', 'calendar', 'tabl', 'term', 'date', 'add', 'drop', 'deadlin', 'withdraw']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords', quiet=True)  # Suppress download warnings\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def normalize_tokens(tokens):\n",
    "    return [stemmer.stem(t) for t in tokens if t not in stop_words]\n",
    "\n",
    "# Example: normalize one document\n",
    "norm_tokens = normalize_tokens(tokens)\n",
    "print(norm_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b85b2",
   "metadata": {},
   "source": [
    "Step 4–6: Word2Vec Training & Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "470e79c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 269 tokenized & normalized sentences.\n",
      "Word2Vec model trained with 772 words in vocabulary.\n",
      "\n",
      " Similarity between 'student' and 'support':\n",
      "0.8369573\n",
      "\n",
      " Most similar to 'exam':\n",
      "[('student', 0.815856397151947), ('q', 0.8031508326530457), ('financi', 0.7925630807876587), ('support', 0.778157114982605), ('tutor', 0.7773033380508423), ('result', 0.7696248292922974), ('learn', 0.7675812840461731), ('time', 0.7660902738571167), ('assist', 0.7647890448570251), ('access', 0.7647355198860168)]\n",
      "\n",
      " Analogy: student + success - stress ≈ ?\n",
      "[('support', 0.6463963985443115), ('cours', 0.6363980770111084), ('learn', 0.6215872168540955), ('ye', 0.619388997554779), ('studi', 0.6171773076057434), ('assign', 0.6165121793746948), ('log', 0.6045294404029846), ('q', 0.6009884476661682), ('assist', 0.5998092293739319), ('academ', 0.5955801010131836)]\n",
      "\n",
      " Odd one out: ['exam', 'assignment', 'deadline', 'cafeteria']\n",
      "cafeteria\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Step 4: Prepare corpus for Word2Vec\n",
    "corpus = []\n",
    "\n",
    "for doc in documents:\n",
    "    # Split by sentence using `.split('.')` to avoid nltk sentence tokenizer errors\n",
    "    sentences = doc.split('.')\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenize(sentence)  # from your earlier function\n",
    "        norm = normalize_tokens(tokens)  # from your earlier function\n",
    "        if norm:  # only add non-empty sentences\n",
    "            corpus.append(norm)\n",
    "\n",
    "print(f\"Prepared {len(corpus)} tokenized & normalized sentences.\")\n",
    "\n",
    "# Step 5: Train Word2Vec model\n",
    "model = Word2Vec(\n",
    "    sentences=corpus,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,  # use 1 to ensure all words are included\n",
    "    sg=1,         # skip-gram model\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Word2Vec model trained with\", len(model.wv.index_to_key), \"words in vocabulary.\")\n",
    "\n",
    "# Step 6: Run Example Queries\n",
    "def safe_query(description, func):\n",
    "    print(f\"\\n {description}\")\n",
    "    try:\n",
    "        print(func())\n",
    "    except KeyError as e:\n",
    "        print(\" Word not found in vocabulary:\", e)\n",
    "\n",
    "safe_query(\"Similarity between 'student' and 'support':\",\n",
    "           lambda: model.wv.similarity('student', 'support'))\n",
    "\n",
    "safe_query(\"Most similar to 'exam':\",\n",
    "           lambda: model.wv.most_similar('exam'))\n",
    "\n",
    "safe_query(\"Analogy: student + success - stress ≈ ?\",\n",
    "           lambda: model.wv.most_similar(positive=['student', 'success'], negative=['stress']))\n",
    "\n",
    "safe_query(\"Odd one out: ['exam', 'assignment', 'deadline', 'cafeteria']\",\n",
    "           lambda: model.wv.doesnt_match(['exam', 'assignment', 'deadline', 'cafeteria']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcb5032",
   "metadata": {},
   "source": [
    "Optional: Explore Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cb3b7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample vocab words: ['q', 'student', 'academ', 'may', 'card', 'servic', 'cours', 'program', 'one', 'support', 'polici', 'grade', 'exam', 'access', 'faculti', 'lab', 'portal', 'learn', 'document', 'time']\n"
     ]
    }
   ],
   "source": [
    "# List a few words in the vocabulary\n",
    "print(\"\\n Sample vocab words:\", model.wv.index_to_key[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac3d95a",
   "metadata": {},
   "source": [
    "# Build Response Vectors from Your Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a34b649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created embeddings for 5 student queries.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 4: Prepare corpus for Word2Vec (you already did this)\n",
    "# corpus = [...]  # your preprocessed, tokenized, normalized sentences list\n",
    "\n",
    "# Step 5: Train Word2Vec model (you already did this)\n",
    "# model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, sg=1, seed=42)\n",
    "\n",
    "# Step 6: Prepare your student queries or knowledge base responses\n",
    "student_queries = [\n",
    "    \"What is the exam schedule for next semester?\",\n",
    "    \"How can I apply for student support services?\",\n",
    "    \"Where is the library located on campus?\",\n",
    "    \"What are the deadlines for assignment submission?\",\n",
    "    \"How to contact the academic advisor?\"\n",
    "]\n",
    "\n",
    "# Preprocess queries same as training data\n",
    "def preprocess_text(text):\n",
    "    tokens = tokenize(text)\n",
    "    norm_tokens = normalize_tokens(tokens)\n",
    "    return norm_tokens\n",
    "\n",
    "processed_queries = [preprocess_text(query) for query in student_queries]\n",
    "\n",
    "# Get sentence embedding by averaging word vectors\n",
    "def get_sentence_vector(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Create embeddings for all queries\n",
    "query_vectors = np.array([get_sentence_vector(tokens, model) for tokens in processed_queries])\n",
    "\n",
    "print(f\"✅ Created embeddings for {len(query_vectors)} student queries.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1663b28d",
   "metadata": {},
   "source": [
    "### 🧭 Matching User Queries via Cosine Similarity\n",
    "\n",
    "When a user asks a question like \"What time is the next bus to Toronto?\", we embed the question and compare it against all stored responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9dfb00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: How do I submit my assignment on time?\n",
      "\n",
      "Best matches:\n",
      "- (0.90) How can I apply for student support services?\n",
      "- (0.84) How to contact the academic advisor?\n",
      "- (0.84) What are the deadlines for assignment submission?\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# User query to match against student queries\n",
    "user_query = \"How do I submit my assignment on time?\"\n",
    "\n",
    "# Preprocess the user query same as training data\n",
    "def preprocess_text(text):\n",
    "    tokens = tokenize(text)\n",
    "    norm_tokens = normalize_tokens(tokens)\n",
    "    return norm_tokens\n",
    "\n",
    "user_tokens = preprocess_text(user_query)\n",
    "\n",
    "# Get embedding for user query\n",
    "def get_sentence_vector(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "user_vec = get_sentence_vector(user_tokens, model)\n",
    "\n",
    "# Compute cosine similarity with precomputed query_vectors\n",
    "similarities = cosine_similarity([user_vec], query_vectors)[0]\n",
    "\n",
    "# Rank student queries by similarity\n",
    "ranked_indices = similarities.argsort()[::-1]\n",
    "\n",
    "print(\"User Query:\", user_query)\n",
    "print(\"\\nBest matches:\")\n",
    "\n",
    "for i in ranked_indices[:3]:  # top 3 matches\n",
    "    print(f\"- ({similarities[i]:.2f}) {student_queries[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a3645",
   "metadata": {},
   "source": [
    "### 🧭 Extension: Building a Semantic Chatbot for Student advisor chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "049f3fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_response(user_input, response_texts, response_vecs, model):\n",
    "    def sentence_vector(sentence):\n",
    "        # Use your tokenizer and normalizer instead of simple lower & split\n",
    "        tokens = normalize_tokens(tokenize(sentence))\n",
    "        vecs = [model.wv[word] for word in tokens if word in model.wv]\n",
    "        return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "\n",
    "    user_vec = sentence_vector(user_input)\n",
    "    scores = cosine_similarity([user_vec], response_vecs)[0]\n",
    "    top_index = scores.argmax()\n",
    "    return response_texts[top_index], scores[top_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd3e79b",
   "metadata": {},
   "source": [
    "### 🧭 Running the Chatbot in a Loop\n",
    "\n",
    "We now simulate a basic chatbot interface in a terminal-style loop. The user types a query, the assistant finds the best-matching predefined response using cosine similarity.\n",
    "\n",
    "This demonstrates vector space proximity in action in a conversational system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58aa42bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎓 Student Support Bot (Jupyter Edition)\n",
      "Ask questions about student support. Type 'exit' to quit.\n",
      "\n",
      "Bot: # ONE Card FAQ\n",
      "\n",
      "Q: What is a ONE Card?\n",
      "A: The ONE Card is Conestoga’s official campus ID for students, staff, and faculty. It provides access to buildings, labs, library services, photocopying, and sometimes meal purchases depending on your campus.\n",
      "\n",
      "Q: Who is eligible for one?\n",
      "A: All registered students, including full-time, part-time, Continuing Education, and some affiliate learners, receive a ONE Card once admitted and have registered or paid a deposit.\n",
      "\n",
      "Q: How do I get my card photo uploaded?\n",
      "A: Log into the ONE Card Portal with your student credentials, follow prompts to upload a clear, front-facing photo, and submit. It should meet specified guidelines (white background, 2″×2″ passport style).\n",
      "\n",
      "Q: When will my ONE Card be ready?\n",
      "A: After uploading and submission, allow 1–2 business days for processing. You'll receive an email notification. Physical pick‑up is available at campus offices.\n",
      "\n",
      "Q: Where can I pick up the card?\n",
      "A: Depending on your campus: Kitchener‑Doon, Waterloo, Cambridge, Guelph, etc. See ONE Card site for specific locations. Bring photo ID & student number.\n",
      "\n",
      "Q: What if I don't upload a photo?\n",
      "A: Your ONE Card production will be delayed. You must upload a photo and pick it up before accessing services such as library, labs, or exams.\n",
      "\n",
      "Q: Can I pick up someone else's card?\n",
      "A: No. For security, you must pick up your own card—no exceptions. Identification validation ensures student records are correct.\n",
      "\n",
      "Q: How do I activate the ONE Card?\n",
      "A: Activation may involve visiting a staff member’s station or logging into the portal to confirm activation. It becomes active once issued.\n",
      "\n",
      "Q: What services use the ONE Card?\n",
      "A: Building access (depending on program); library services; photocopy/printing; labs; some campuses enable purchases in cafeterias or vending machines; event check-ins.\n",
      "\n",
      "Q: What should I do if I lose the card?\n",
      "A: Report it immediately via the ONE Card Portal or Student Central to suspend it. Then apply for a replacement. Fees may apply.\n",
      "\n",
      "Q: Is there a replacement fee?\n",
      "A: Fees vary but typically range from $20–$35. Temporary cards may be issued while a replacement is processed.\n",
      "\n",
      "Q: How long does it take to get a replacement?\n",
      "A: Replacement processing typically takes 1–3 business days after ordering and paying the fee. Temporary cards allow limited access.\n",
      "\n",
      "Q: What if it’s damaged?\n",
      "A: Submit a damage report in the portal. If swipe/magnetic strip is broken, request replacement. If damage is minor (scratches), temporary replacement may not be required.\n",
      "\n",
      "Q: Can I load money onto the ONE Card?\n",
      "A: Some campuses integrate meal or snack account funds via the “eConestoga” system. Check campus specific links for loading funds via cash, debit, card, or online portal.\n",
      "\n",
      "Q: Can I check my card balance?\n",
      "A: Yes. Log into the ONE Card or eConestoga portal to view current balance, transaction details, and expiry dates.\n",
      "\n",
      "Q: Does the card expire?\n",
      "A: ONE Cards expire after your program end date or convocation month. Expired cards may not be activated again and must be replaced for future enrollment.\n",
      "\n",
      "Q: What if I graduate and later return?\n",
      "A: After convocation, your card is deactivated. If you return to studies, contact onecard@conestogac.on.ca to reactivate or request renewed card.\n",
      "\n",
      "Q: Is there a photo approval requirement?\n",
      "A: Yes, the photo must clearly show your face, taken recently (within six months), with no sunglasses, filters, hats (unless religious). Smiling is okay; full face must be visible.\n",
      "\n",
      "Q: Can I change my photo?\n",
      "A: Yes, log into portal to upload a new photo. Processing time is ~1 business day. Notify card office if old card needs reprinting.\n",
      "\n",
      "Q: What should I do if I'm traveling abroad?\n",
      "A: The ONE Card is campus-use; it won’t work for international travel or financial transactions. Use your regular credit/debit card instead.\n",
      "\n",
      "  \n",
      " (similarity: 0.51)\n",
      "\n",
      "Bot: Good luck with your studies! 👋\n"
     ]
    }
   ],
   "source": [
    "print(\"🎓 Student Support Bot (Jupyter Edition)\")\n",
    "print(\"Ask questions about student support. Type 'exit' to quit.\\n\")\n",
    "\n",
    "# Precompute response vectors for your documents (or pre-defined answers)\n",
    "response_texts = documents  # or use a list of prepared answer strings if you have one\n",
    "\n",
    "def sentence_vector(sentence):\n",
    "    tokens = normalize_tokens(tokenize(sentence))\n",
    "    vecs = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "\n",
    "response_vectors = np.array([sentence_vector(resp) for resp in response_texts])\n",
    "\n",
    "max_turns = 10\n",
    "turn = 0\n",
    "\n",
    "while turn < max_turns:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Bot: Good luck with your studies! 👋\")\n",
    "        break\n",
    "\n",
    "    reply, score = get_best_response(user_input, response_texts, response_vectors, model)\n",
    "    print(f\"Bot: {reply} (similarity: {score:.2f})\\n\")\n",
    "    \n",
    "    turn += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb5921ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎓 Student Support Bot (Jupyter Edition)\n",
      "Ask questions about student support. Type 'exit' to quit.\n",
      "\n",
      "Bot: Q: Is there a replacement fee?\n",
      "A: Fees vary but typically range from $20–$35. Temporary cards may be issued while a replacement is processed. (similarity: 0.58)\n",
      "\n",
      "Bot: Good luck with your studies! 👋\n"
     ]
    }
   ],
   "source": [
    "print(\"🎓 Student Support Bot (Jupyter Edition)\")\n",
    "print(\"Ask questions about student support. Type 'exit' to quit.\\n\")\n",
    "\n",
    "# --- STEP 1: Flatten grouped FAQ into individual entries (if needed) ---\n",
    "# You must split documents into individual answers if they're grouped under a heading\n",
    "# For example, if each document has multiple Q&A lines, split them by '\\n\\n' or similar\n",
    "flat_documents = []\n",
    "for doc in documents:\n",
    "    parts = doc.strip().split(\"\\n\\n\")\n",
    "    for part in parts:\n",
    "        if part.strip():\n",
    "            flat_documents.append(part.strip())\n",
    "\n",
    "# --- STEP 2: Sentence embedding ---\n",
    "def sentence_vector(sentence):\n",
    "    tokens = normalize_tokens(tokenize(sentence))\n",
    "    vecs = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "\n",
    "response_vectors = np.array([sentence_vector(resp) for resp in flat_documents])\n",
    "\n",
    "# --- STEP 3: Best match based on cosine similarity ---\n",
    "def get_best_response(user_input, responses, vectors):\n",
    "    user_vec = sentence_vector(user_input)\n",
    "    similarities = np.dot(vectors, user_vec) / (\n",
    "        np.linalg.norm(vectors, axis=1) * np.linalg.norm(user_vec) + 1e-10\n",
    "    )\n",
    "    best_idx = int(np.argmax(similarities))\n",
    "    return responses[best_idx], float(similarities[best_idx])\n",
    "\n",
    "# --- STEP 4: Chat loop ---\n",
    "max_turns = 10\n",
    "turn = 0\n",
    "\n",
    "while turn < max_turns:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Bot: Good luck with your studies! 👋\")\n",
    "        break\n",
    "\n",
    "    reply, score = get_best_response(user_input, flat_documents, response_vectors)\n",
    "    print(f\"Bot: {reply} (similarity: {score:.2f})\\n\")\n",
    "\n",
    "    turn += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017d7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎓 Student Success Advisor Bot\n",
      "Ask a question (type 'exit' to quit)\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sbert_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot: Goodbye and good luck! 🎓\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m user_vec \u001b[38;5;241m=\u001b[39m sbert_model\u001b[38;5;241m.\u001b[39mencode([user_query])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     11\u001b[0m sims \u001b[38;5;241m=\u001b[39m cosine_similarity([user_vec], response_vectors)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m idx \u001b[38;5;241m=\u001b[39m sims\u001b[38;5;241m.\u001b[39margmax()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sbert_model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = tokenize(text)\n",
    "    return normalize_tokens(tokens)\n",
    "\n",
    "def get_sentence_vector(tokens, model=model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "print(\"🎓 Student Support Chatbot (type 'done' to exit)\\n\")\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"You: \").strip()\n",
    "    \n",
    "    if user_query.lower() in [\"done\", \"exit\", \"quit\"]:\n",
    "        print(\"Bot: Goodbye! Contact student support if needed. 👋\")\n",
    "        break\n",
    "\n",
    "    user_tokens = preprocess_text(user_query)\n",
    "    user_vec = get_sentence_vector(user_tokens)\n",
    "\n",
    "    similarities = cosine_similarity([user_vec], response_vectors)[0]\n",
    "    best_idx = similarities.argmax()\n",
    "    best_score = similarities[best_idx]\n",
    "\n",
    "    if best_score > 0.4:  # You can adjust threshold\n",
    "        print(f\"Bot: {responses[best_idx]} (confidence: {best_score:.2f})\\n\")\n",
    "    else:\n",
    "        print(\"Bot: 🤖 Sorry, I’m not sure. Please contact your academic advisor.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
