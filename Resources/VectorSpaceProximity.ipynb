{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651e4f56",
   "metadata": {},
   "source": [
    "# üß† NLP Foundations Workshop: Vector Space Proximity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ddb017",
   "metadata": {},
   "source": [
    "### üîπ Introduction to Vector Space Proximity\n",
    "\n",
    "A large majority of the data on the Internet is **unstructured**, for example: social media posts, emails, images, videos and audio files.\n",
    "\n",
    "If we want to **persist** all these media in a database, we may add **metadata** about them, such as file type or creation date timestamp, or we could  **tag** each file, or parts of it, so they are easy to search for. This is because it would be very difficult to identify them based on their low-level (byte) representations.\n",
    "\n",
    "But, what if we want to make the process fully automated (i.e., remove the need to manually add features, like tags, to each media item)? We need another way to represent the semantics of digital media.\n",
    "\n",
    "That is the reason why in **Information Retrieval (IR)** and **Natural Language Processing (NLP)**, we often represent documents and queries as **vectors** in a **high-dimensional space**, where:\n",
    "\n",
    "* Each **dimension** corresponds to a **unique term** in the vocabulary.\n",
    "* A **document** is represented by a **point** or a **vector** in the space.\n",
    "* A **vector** is a list of weights (e.g., term frequencies, TF-IDF values) that describe the presence or importance of terms in a document or query.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìò Example 1: \"Rich\" and \"Poor\" Axes\n",
    "\n",
    "![Vector Space Example: \"Rich\" and \"Poor\" Axes\"](./images/Fig1_CartesianVectorSpace.png)\n",
    "\n",
    "Suppose our vocabulary only has two terms:\n",
    "\n",
    "* `\"rich\"`\n",
    "* `\"poor\"`\n",
    "\n",
    "These two terms define a **2D Cartesian space**:\n",
    "\n",
    "* The **x-axis** corresponds to the term **\"rich\"**.\n",
    "* The **y-axis** corresponds to the term **\"poor\"**.\n",
    "\n",
    "Each document is represented as a vector in this space:\n",
    "\n",
    "* A document with many occurrences of ‚Äúpoor‚Äù and none of ‚Äúrich‚Äù lies near the **y-axis**.\n",
    "* A document that mentions both ‚Äúrich‚Äù and ‚Äúpoor‚Äù lies in the **first quadrant**.\n",
    "* A document with only ‚Äúrich‚Äù is aligned along the **x-axis**.\n",
    "\n",
    "The **query vector** $q = \\{\\text{\"rich\"}, \\text{\"poor\"}\\}$ points in the direction of interest for the search engine.\n",
    "\n",
    "### üîπ Euclidean Distance and Its Limitations\n",
    "\n",
    "One might assume we can measure similarity using **Euclidean distance**:\n",
    "\n",
    "$$\n",
    "\\text{Euclidean}( \\vec{q}, \\vec{d} ) = \\sqrt{ \\sum_{i=1}^{n} (q_i - d_i)^2 }\n",
    "$$\n",
    "\n",
    "However, this has problems in practice:\n",
    "\n",
    "* If document $d_2$ contains more occurrences of both ‚Äúrich‚Äù and ‚Äúpoor‚Äù than the query, its vector will have a **longer length**.\n",
    "* As seen in the diagram, even though $d_2$ has strong content overlap with the query $q$, it may still be **further away** in Euclidean terms than unrelated documents like $d_3$.\n",
    "* This happens because **magnitude dominates**, not direction.\n",
    "\n",
    "### üîπ Angle as Similarity ‚Üí Cosine Similarity\n",
    "\n",
    "To solve this, we focus on **vector direction**, not length. We measure **angle** between the document and query vectors using **Cosine Similarity**:\n",
    "\n",
    "$$\n",
    "\\cos(\\vec{q}, \\vec{d}) = \\frac{ \\vec{q} \\cdot \\vec{d} }{ \\|\\vec{q}\\| \\cdot \\|\\vec{d}\\| }\n",
    "= \\frac{ \\sum_{i=1}^{n} q_i \\cdot d_i }{ \\sqrt{ \\sum_{i=1}^{n} q_i^2 } \\cdot \\sqrt{ \\sum_{i=1}^{n} d_i^2 } }\n",
    "$$\n",
    "\n",
    "* This gives us a similarity score from **0 (orthogonal)** to **1 (identical direction)**.\n",
    "* Longer documents that are semantically aligned still get **high similarity**.\n",
    "\n",
    "### üîπ Why Cosine Similarity Works Better\n",
    "\n",
    "* **Angle** captures **semantic alignment**.\n",
    "* It is **not affected** by document length or repetition.\n",
    "* Example: duplicating document $d$ to make $d'$ will increase Euclidean distance ‚Äî but **cosine similarity remains 1**.\n",
    "\n",
    "Cosine similarity is at the core of:\n",
    "\n",
    "* **Search ranking**\n",
    "* **Embedding-based retrieval**\n",
    "* **LLM scoring and attention mechanisms**\n",
    "\n",
    "Sample code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc3dddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  ['rich poor']\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Document",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Cosine Similarity with Query",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "1b81cf06-e28f-45d5-8a23-e14013328625",
       "rows": [
        [
         "0",
         "Doc2",
         "0.7071067811865475"
        ],
        [
         "1",
         "Doc1",
         "0.0"
        ],
        [
         "2",
         "Doc3",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Cosine Similarity with Query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Doc2</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Doc1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Doc3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Document  Cosine Similarity with Query\n",
       "0     Doc2                      0.707107\n",
       "1     Doc1                      0.000000\n",
       "2     Doc3                      0.000000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define the documents and the query\n",
    "documents = [\n",
    "    \"Ranks of starving poets swell\",       # d1\n",
    "    \"Rich poor gap grows\",                 # d2\n",
    "    \"Record baseball salaries in 2010\"     # d3\n",
    "]\n",
    "\n",
    "query = [\"rich poor\"]                     # q\n",
    "\n",
    "# Create a CountVectorizer to convert text to term frequency vectors\n",
    "vectorizer = CountVectorizer()\n",
    "doc_vectors = vectorizer.fit_transform(documents + query).toarray()\n",
    "\n",
    "# Separate vectors\n",
    "doc_matrix = doc_vectors[:3]  # d1, d2, d3\n",
    "query_vector = doc_vectors[3].reshape(1, -1)  # q\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_similarities = cosine_similarity(query_vector, doc_matrix).flatten()\n",
    "\n",
    "# Create a DataFrame to show results\n",
    "df = pd.DataFrame({\n",
    "    'Document': ['Doc1', 'Doc2', 'Doc3'],\n",
    "    'Cosine Similarity with Query': cosine_similarities\n",
    "})\n",
    "\n",
    "# Sort for clarity\n",
    "print(\"Query: \", query)\n",
    "df.sort_values(by='Cosine Similarity with Query', ascending=False, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the result\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25563bf8",
   "metadata": {},
   "source": [
    "### üìò Example 2: Word Vectors in a Small Corpus\n",
    "\n",
    "Let's start with a small corpus of just six words, each represented by a vector in 3D space:\n",
    "\n",
    "```plaintext\n",
    "CAT     ‚Üí [ 0.2, -0.4,  0.7]\n",
    "DOG     ‚Üí [ 0.6,  0.1,  0.5]\n",
    "APPLE   ‚Üí [ 0.8, -0.2, -0.3]\n",
    "ORANGE  ‚Üí [ 0.7, -0.1, -0.6]\n",
    "HAPPY   ‚Üí [-0.5,  0.9,  0.2]\n",
    "SAD     ‚Üí [ 0.4, -0.7, -0.5]\n",
    "```\n",
    "\n",
    "Each term is represented by a **vector in 3D space**.\n",
    "\n",
    "### üîç Observations\n",
    "\n",
    "- Words with **similar meanings** tend to have **similar vector representations**.\n",
    "  - For example, **APPLE** and **ORANGE** are close in vector space, reflecting their semantic similarity.\n",
    "\n",
    "- Words with **opposite meanings** tend to have **vectors pointing in opposite directions**.\n",
    "  - For instance, **HAPPY** and **SAD** have contrasting vectors, indicating their opposing emotional tones.\n",
    "\n",
    "![3D Visualizationof Word Vectors](./images/Fig2_3DVisualizationWordVectors.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c8a0ea",
   "metadata": {},
   "source": [
    "Vector representations are also called **Embeddings**.\n",
    "\n",
    "There are several approaaches to how **word embedding methods** generate effective vector representations. \n",
    "\n",
    "One of them is **frequency-based embeddings**, word representations that are derived from the frequency of words in a corpus. They are based on the idea that the **importance** or the **significance** of a word can be inferred from **how frequently it occurs in the text**. One such embedding is called **Term Frequency - Inverse Document Frequency** or **TF-IDF**. \n",
    "\n",
    "TF-IDF highlights words that are frequent within a specific document but are rare across the entire corpus. For example, in a document about music, it would emphasize words such as **rap**, **disco**, **pop**, **rock**. On the other hand, pronouns would receive a low TF-IDF score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8f764",
   "metadata": {},
   "source": [
    "There are various models for generating word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d42c1ba",
   "metadata": {},
   "source": [
    "### üîπ Word Embeddings with Word2Vec\n",
    "\n",
    "**Word2Vec** is one of the most influential models for learning **dense vector representations** of words, also known as **embeddings**.\n",
    "\n",
    "Unlike frequency-based models like TF-IDF, Word2Vec uses a **neural network** to learn word vectors such that **similar words have similar embeddings**.\n",
    "\n",
    "There are two main architectures:\n",
    "\n",
    "* **CBOW (Continuous Bag of Words)**: Predicts a word from its context.\n",
    "* **Skip-gram**: Predicts context words from a target word.\n",
    "\n",
    "Both approaches rely on the **distributional hypothesis**: words that appear in similar contexts tend to have similar meanings.\n",
    "\n",
    "### üíª Code Challenge: Learn Word Embeddings Using Word2Vec\n",
    "\n",
    "#### üöÄ Your Task:\n",
    "\n",
    "Write Python code that:\n",
    "\n",
    "1. Prepares a small corpus of tokenized sentences.\n",
    "2. Trains a **Word2Vec** model on this corpus using Gensim.\n",
    "3. Displays the vector representation for a few words.\n",
    "4. Finds the most similar words to a chosen term.\n",
    "\n",
    "#### üìö Hints:\n",
    "\n",
    "* Use `from gensim.models import Word2Vec`\n",
    "* Tokenize your corpus as a list of word lists (sentences).\n",
    "* Try: `model.wv['word']`, `model.wv.most_similar('word')`\n",
    "\n",
    "**Example Questions to Explore:**\n",
    "\n",
    "* What is the shape of a word vector?\n",
    "* Which words are closest to \"learning\", \"data\", or \"model\"?\n",
    "* Can Word2Vec capture analogies (e.g., \"king\" - \"man\" + \"woman\")?\n",
    "\n",
    "Try it out and see what your model learns! üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01b150d",
   "metadata": {},
   "source": [
    "#### üöÄ Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d6e113e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for : learning\n",
      "[ 5.4432148e-05 -1.2916273e-02 -3.1752426e-02  4.2684775e-02\n",
      "  2.8163580e-02  1.4394796e-02 -9.7623421e-03  3.2266378e-02\n",
      "  4.5307353e-03 -5.6793080e-03 -4.9702777e-03 -2.7299756e-02\n",
      " -4.0785067e-02  5.4709227e-03  3.8802378e-02 -4.3621615e-02\n",
      "  3.5840165e-02  3.2752793e-02 -2.2315878e-02  1.3174340e-02]\n",
      "Shape: (20,)\n",
      "----------------------------------------\n",
      "Vector for : data\n",
      "[-0.04108218  0.0274296   0.01548314 -0.00610298 -0.00672426  0.03585006\n",
      " -0.0414423   0.01973772 -0.02986624 -0.04060877  0.00266196  0.0475366\n",
      "  0.0235936   0.02612126  0.02178242  0.02863825  0.00130401 -0.03724516\n",
      "  0.03397556 -0.00496576]\n",
      "Shape: (20,)\n",
      "----------------------------------------\n",
      "Vector for : model\n",
      "[ 4.9803518e-02  2.7741602e-02 -1.7956838e-02  4.7178898e-02\n",
      " -7.2493975e-05  8.0637117e-05 -6.6362475e-03 -3.5607822e-02\n",
      "  4.3438919e-02 -4.8612114e-02 -3.2571662e-02 -2.7037278e-02\n",
      " -2.2066751e-02 -3.6817949e-02 -4.8198365e-02  1.7767431e-02\n",
      "  4.6933107e-02 -3.7817143e-02 -2.0344283e-02  6.2751846e-04]\n",
      "Shape: (20,)\n",
      "----------------------------------------\n",
      "Vector for : king\n",
      "[-0.02120683  0.03294286 -0.02494869  0.02968171 -0.00369923 -0.02673593\n",
      " -0.01657558  0.00307696  0.02651471  0.01060158  0.04332238  0.03677389\n",
      "  0.02533535  0.01031072 -0.02101442 -0.00874284 -0.03811057 -0.0125816\n",
      "  0.04264722 -0.0074118 ]\n",
      "Shape: (20,)\n",
      "----------------------------------------\n",
      "Vector for : queen\n",
      "[-0.03862712  0.0151931   0.04626148  0.03674906 -0.02259236 -0.00461031\n",
      "  0.00310165 -0.02521605 -0.0231308  -0.02633377  0.04945813  0.02460142\n",
      "  0.03408563  0.03165687  0.04591434 -0.03947219  0.03816324 -0.04334412\n",
      " -0.0024579   0.00944336]\n",
      "Shape: (20,)\n",
      "----------------------------------------\n",
      "Most similar to :\n",
      "[('embeddings', 0.5074038505554199), ('word', 0.347505122423172), ('popular', 0.3418503701686859), ('learn', 0.32856056094169617), ('networks', 0.2672286331653595), ('neural', 0.21758729219436646), ('woman', 0.20528872311115265), ('semantics', 0.20494480431079865), ('are', 0.2035827487707138), ('science', 0.13954637944698334)]\n",
      "----------------------------------------\n",
      "Most similar to :\n",
      "[('queen', 0.4444059133529663), ('on', 0.3581668436527252), ('representations', 0.31497621536254883), ('capture', 0.2779500484466553), ('deep', 0.2596377432346344), ('king', 0.25938111543655396), ('popular', 0.2559094727039337), ('uses', 0.18691381812095642), ('is', 0.17467544972896576), ('powerful', 0.16602864861488342)]\n",
      "----------------------------------------\n",
      "Most similar to :\n",
      "[('word', 0.40461400151252747), ('deep', 0.3753891587257385), ('semantics', 0.29687780141830444), ('models', 0.24514545500278473), ('embeddings', 0.23411041498184204), ('fun', 0.20774449408054352), ('man', 0.1701592355966568), ('neural', 0.16851665079593658), ('python', 0.14283740520477295), ('learning', 0.08441723138093948)]\n",
      "----------------------------------------\n",
      "Most similar to :\n",
      "[('semantics', 0.36475855112075806), ('depends', 0.3190699517726898), ('science', 0.308027058839798), ('powerful', 0.28583985567092896), ('data', 0.25938111543655396), ('machine', 0.23754160106182098), ('representations', 0.19705672562122345), ('queen', 0.18593956530094147), ('is', 0.1213439330458641), ('man', 0.10342966020107269)]\n",
      "----------------------------------------\n",
      "Analogy: king - man + woman = ?\n",
      "[('fun', 0.35740363597869873), ('semantics', 0.35546138882637024), ('depends', 0.35479703545570374), ('data', 0.31317758560180664), ('science', 0.20852580666542053), ('powerful', 0.17998428642749786), ('are', 0.1563640832901001), ('machine', 0.10168661922216415), ('python', 0.061376530677080154), ('queen', 0.05523523688316345)]\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec Code Challenge\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 1. Prepare a small corpus of tokenized sentences\n",
    "corpus = [\n",
    "    ['machine', 'learning', 'is', 'fun'],\n",
    "    ['deep', 'learning', 'models', 'are', 'powerful'],\n",
    "    ['data', 'science', 'uses', 'models'],\n",
    "    ['word', 'embeddings', 'capture', 'semantics'],\n",
    "    ['neural', 'networks', 'learn', 'representations'],\n",
    "    ['model', 'accuracy', 'depends', 'on', 'data'],\n",
    "    ['king', 'queen', 'man', 'woman'],\n",
    "    ['python', 'is', 'popular', 'for', 'data', 'science']\n",
    "]\n",
    "\n",
    "# 2. Train a Word2Vec model\n",
    "model = Word2Vec(sentences=corpus, vector_size=20, window=3, min_count=1, sg=1, seed=42)\n",
    "\n",
    "# 3. Display vector representations for a few words\n",
    "words_to_show = ['learning', 'data', 'model', 'king', 'queen']\n",
    "for word in words_to_show:\n",
    "    print(f'Vector for :', word)\n",
    "    print(model.wv[word])\n",
    "    print('Shape:', model.wv[word].shape)\n",
    "    print('-'*40)\n",
    "\n",
    "# 4. Find most similar words to a chosen term\n",
    "chosen_terms = ['learning', 'data', 'model', 'king']\n",
    "for term in chosen_terms:\n",
    "    print(f'Most similar to :')\n",
    "    print(model.wv.most_similar(term))\n",
    "    print('-'*40)\n",
    "\n",
    "# 5. Analogy: king - man + woman\n",
    "print('Analogy: king - man + woman = ?')\n",
    "print(model.wv.most_similar(positive=['king', 'woman'], negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af15e58",
   "metadata": {},
   "source": [
    "üß† What's Really Going On?\n",
    "When you train a Word2Vec model:\n",
    "\n",
    "* Each word becomes a vector of real numbers.\n",
    "* These vectors live in a multi-dimensional space (in your case, 20D).\n",
    "* The proximity (closeness) between words in that space reflects similarity of meaning or context.\n",
    "\n",
    "üì¶ Example: Word Vector Breakdown\n",
    "Here's one word vector you saw:\n",
    "\n",
    "**Vector for 'learning':**\n",
    "**[ 5.44e-05, -0.0129, -0.0317, ..., 0.0131 ]**\n",
    "\n",
    "* This is a 20-dimensional vector: each number is a \"coordinate\" of the word \"learning\" in this abstract space.\n",
    "* We can't visualize 20D, but you can imagine each vector like an arrow pointing to a specific location in space.\n",
    "* Similar words will have vectors pointing in similar directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9f31bd",
   "metadata": {},
   "source": [
    "**Vector for 'king':**\n",
    "**[-0.02120683  0.03294286 -0.02494869  0.02968171 -0.00369923 -0.02673593... ]**\n",
    "\n",
    "* This is a 20-dimensional vector: each number is a \"coordinate\" of the word \"king\" in this abstract space.\n",
    "* We can't visualize 20D, but you can imagine each vector like an arrow pointing to a specific location in space.\n",
    "* Similar words will have vectors pointing in similar directions.\n",
    "\n",
    "Now, right after the vector for 'king' we can see:\n",
    "\n",
    "**[('semantics', 0.36475855112075806), ('depends', 0.3190699517726898), ('science', 0.308027058839798), ('powerful', 0.28583985567092896), ('data', 0.25938111543655396), ('machine', 0.23754160106182098), ('representations', 0.19705672562122345), ('queen', 0.18593956530094147), ('is', 0.1213439330458641), ('man', 0.10342966020107269)]**\n",
    "\n",
    "This is a list of the top 10 words in your Word2Vec model whose vectors are closest to the word **king**\n",
    "\n",
    "* `semantics` is the most similar (0.365)\n",
    "* `queen` is the 8th most similar (0.186)\n",
    "* `man` is the 10th most similar (0.0103)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f43b0",
   "metadata": {},
   "source": [
    "üéØ Visual Analogy (2D Simplified)\n",
    "\n",
    "Finally, we see:\n",
    "\n",
    "**Analogy: king - man + woman = ?**\n",
    "[('fun', 0.35740363597869873), ('semantics', 0.35546138882637024), ('depends', 0.35479703545570374), ('data', 0.31317758560180664), ('science', 0.20852580666542053), ('powerful', 0.17998428642749786), ('are', 0.1563640832901001), ('machine', 0.10168661922216415), ('python', 0.061376530677080154), ('queen', 0.05523523688316345)]\n",
    "\n",
    "This is a list of the top 10 words in your Word2Vec model whose vectors are closest to the result of the analogy.\n",
    "\n",
    "Let‚Äôs simplify it to 2D for illustration:\n",
    "\n",
    "```plaintext\n",
    "\n",
    "                ‚ñ≤  \"queen\"\n",
    "                |\n",
    "                |\n",
    "         \"king\" ‚óè\n",
    "                |       ‚óè \"woman\"\n",
    "                |     ‚óè\n",
    "                |   ‚óè  \"fun\"\n",
    "                | ‚óè\n",
    "  --------------‚óè----------------‚ñ∂\n",
    "             \"man\"             (some other word)\n",
    "```\n",
    "\n",
    "In the Vector Arithmetic associated with `king - man + woman ‚âà ?`\n",
    "\n",
    "[('fun', 0.35740363597869873), ('semantics', 0.35546138882637024), ('depends', 0.35479703545570374), ('data', 0.31317758560180664), ('science', 0.20852580666542053), ('powerful', 0.17998428642749786), ('are', 0.1563640832901001), ('machine', 0.10168661922216415), ('python', 0.061376530677080154), ('queen', 0.05523523688316345)]\n",
    "\n",
    "Each tuple is:\n",
    "\n",
    "* A word from your vocabulary (e.g., 'fun')\n",
    "* Its cosine similarity score with the resulting vector (e.g., 0.357)\n",
    "* So 'fun' is the most similar word to the computed vector, but 'queen' is only ranked 10th, with a low similarity of 0.055.\n",
    "\n",
    "> Why is \"queen\" not at the top?\n",
    "\n",
    "* Your corpus is small, so the model doesn't have enough context to truly learn that \"king\" and \"queen\" are related.\n",
    "* On a larger corpus, you'd likely see queen rank first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cf367a",
   "metadata": {},
   "source": [
    "ü§î What Does king - man + woman ‚âà ? Really Mean?\n",
    "\n",
    "* It‚Äôs a semantic equation based on the idea:\n",
    "* \"king is to man\" as \"queen is to woman\"\n",
    "\n",
    "In vector form:\n",
    "\n",
    "**vec(\"king\") - vec(\"man\") + vec(\"woman\") ‚âà vec(\"queen\")**\n",
    "\n",
    "\n",
    "> This directional arithmetic is the magic behind Word2Vec's analogy power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bfe29",
   "metadata": {},
   "source": [
    "üîç Why is the word 'king' represented as a 20-dimensional vector? Why not 10 dimensions?\n",
    "\n",
    "Because we explicitly set the number of dimensions when we trained the Word2Vec model:\n",
    "```Python\n",
    "model = Word2Vec(sentences=corpus, vector_size=20, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e895b",
   "metadata": {},
   "source": [
    "* The vector_size=20 parameter defines the dimensionality of the word embeddings.\n",
    "* This is not learned from data ‚Äî it‚Äôs a hyperparameter you choose before training.\n",
    "* You could have chosen vector_size=10, 50, 100, etc.\n",
    "\n",
    "ü§î How Do You Choose the Right Number?\n",
    "Smaller vector sizes (like 10 or 20) ‚Üí faster training, but may lose semantic richness.\n",
    "\n",
    "Larger sizes (like 100 or 300) ‚Üí capture more subtle patterns, but need more data and risk overfitting on small corpora.\n",
    "\n",
    "Common defaults:\n",
    "\n",
    "* vector_size=100 for small-medium corpora\n",
    "* vector_size=300 is often used in pretrained embeddings (like Google News)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74611586",
   "metadata": {},
   "source": [
    "### üìù Interpreting the Word2Vec Output\n",
    "\n",
    "- **Word Vectors:** The printed vectors for words like 'learning', 'data', 'model', 'king', and 'queen' are dense arrays of numbers. Each vector captures semantic properties learned from the context in the corpus.\n",
    "- **Shape:** The shape of each vector (e.g., 20) matches the 'vector_size' parameter used in training. This is the dimensionality of the embedding space.\n",
    "- **Most Similar Words:** The 'most_similar' results show which words are closest in meaning to the chosen term, based on their vector proximity. For example, words similar to 'learning' might include 'deep', 'machine', or 'models'.\n",
    "- **Analogy (king - man + woman):** This classic test checks if the model can capture analogies. If successful, the top result should be 'queen', showing that the model understands gender relationships in the vector space.\n",
    "\n",
    "Word2Vec embeddings are powerful for capturing semantic relationships, enabling tasks like similarity search, clustering, and analogical reasoning in NLP applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c501a96",
   "metadata": {},
   "source": [
    "### üß† Use case: Word2Vec with Wikipedia Text\n",
    "\n",
    "We now extend our Word2Vec use case by training on real-world text from a **Wikipedia article**. This lets us explore word embeddings on a richer vocabulary and more meaningful context.\n",
    "\n",
    "### üîÑ Process:\n",
    "\n",
    "1. **Download**: Load raw text from Wikipedia using `wikipedia` Python library.\n",
    "2. **Preprocess**: Clean and tokenize sentences.\n",
    "3. **Train**: Build a Word2Vec model on the processed corpus.\n",
    "4. **Query**:\n",
    "   - Similarity between **\"king\"** and **\"queen\"**\n",
    "   - Most similar words to **\"computer\"**\n",
    "   - Analogy: **\"paris\" - \"france\" + \"germany\" ‚âà ?**\n",
    "\n",
    "This demonstrates how embeddings capture context and relationships ‚Äî crucial for tasks like search, question-answering, and even LLMs.\n",
    "\n",
    "> You may need to install `wikipedia` with `pip install wikipedia`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3583259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'punkt' tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to c:\\StudentWork\\Code\\PROG8245\\\n",
      "[nltk_data]     IRBasics_VectorSpaceProximity\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'punkt_tab' tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to c:\\StudentWork\\Code\\PROG8\n",
      "[nltk_data]     245\\IRBasics_VectorSpaceProximity\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Data Paths: ['C:\\\\Users\\\\Eespinosa/nltk_data', 'c:\\\\StudentWork\\\\Code\\\\PROG8245\\\\IRBasics_VectorSpaceProximity\\\\.venv\\\\nltk_data', 'c:\\\\StudentWork\\\\Code\\\\PROG8245\\\\IRBasics_VectorSpaceProximity\\\\.venv\\\\share\\\\nltk_data', 'c:\\\\StudentWork\\\\Code\\\\PROG8245\\\\IRBasics_VectorSpaceProximity\\\\.venv\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\Eespinosa\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', 'c:\\\\StudentWork\\\\Code\\\\PROG8245\\\\IRBasics_VectorSpaceProximity\\\\nltk_data']\n",
      "Contents of nltk_data: ['tokenizers']\n",
      "üîç Similarity between 'learning' and 'reasoning':\n",
      "0.9976537\n",
      "\n",
      "üìö Most similar to 'perception':\n",
      "[('human', 0.9983024001121521), ('google', 0.9980690479278564), ('systems', 0.9980460405349731), ('may', 0.9980018138885498), ('was', 0.9979568719863892), ('has', 0.997938334941864), ('such', 0.9979318380355835), ('trained', 0.9979142546653748), ('some', 0.9979134798049927), ('because', 0.9978792071342468)]\n",
      "\n",
      "üèôÔ∏è Analogy: knowledge - human + robotics ‚âà ?\n",
      "[('real', 0.9951503872871399), ('this', 0.9949238896369934), ('has', 0.9948580861091614), ('many', 0.9947996139526367), ('microsoft', 0.9947676658630371), ('symbolic', 0.9946388602256775), ('some', 0.9946317076683044), ('after', 0.9945502877235413), ('the', 0.9944773316383362), ('where', 0.9944761991500854)]\n",
      "\n",
      "üìö Most similar to 'Meta':\n",
      "[('several', 0.9969318509101868), ('developed', 0.9969031810760498), ('has', 0.9968452453613281), ('such', 0.9968118071556091), ('first', 0.9968109726905823), ('people', 0.9967969059944153), ('between', 0.9967771768569946), ('research', 0.9967686533927917), ('problem', 0.9967467784881592), ('demand', 0.9967355728149414)]\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies (uncomment the line below if not installed)\n",
    "# !pip install wikipedia nltk gensim\n",
    "\n",
    "import wikipedia\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import os\n",
    "\n",
    "# 1. Download Wikipedia text\n",
    "wiki_title = \"Artificial intelligence\"\n",
    "# Fetch the content of the Wikipedia page: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
    "wiki_text = wikipedia.page(wiki_title).content\n",
    "\n",
    "# 2. Preprocess\n",
    "# Ensure 'punkt' is available and nltk_data path is set\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')\n",
    "print(\"Downloading 'punkt' tokenizer...\")\n",
    "nltk.download('punkt', download_dir=nltk_data_path, force=True)\n",
    "print(\"Downloading 'punkt_tab' tokenizer...\")\n",
    "nltk.download('punkt_tab', download_dir=nltk_data_path, force=True)\n",
    "\n",
    "# Always append the custom nltk_data path (if not already present)\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Debugging paths and contents\n",
    "print(\"NLTK Data Paths:\", nltk.data.path)\n",
    "print(\"Contents of nltk_data:\", os.listdir(nltk_data_path))\n",
    "\n",
    "# Ensure wiki_text is not empty and properly encoded\n",
    "if not wiki_text.strip():\n",
    "    raise ValueError(\"The Wikipedia page content is empty. Please check the page title or internet connection.\")\n",
    "try:\n",
    "    wiki_text = wiki_text.encode('utf-8').decode('utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    raise ValueError(\"The Wikipedia page content contains invalid characters and cannot be processed.\")\n",
    "\n",
    "# Split wiki_text into smaller chunks to avoid tokenization issues\n",
    "text_chunks = [wiki_text[i:i+10000] for i in range(0, len(wiki_text), 10000)]\n",
    "sentences = []\n",
    "for chunk in text_chunks:\n",
    "    sentences.extend(sent_tokenize(chunk))\n",
    "\n",
    "tokenized_corpus = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = re.sub(r'[^a-zA-Z]', ' ', sentence)\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "    tokenized_corpus.append(tokens)\n",
    "\n",
    "# 3. Train Word2Vec\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=2, sg=1, seed=42)\n",
    "\n",
    "# 4. Run three example queries\n",
    "print(\"üîç Similarity between 'learning' and 'reasoning':\")\n",
    "print(model.wv.similarity('learning', 'reasoning'))\n",
    "\n",
    "print(\"\\nüìö Most similar to 'perception':\")\n",
    "print(model.wv.most_similar('perception'))\n",
    "\n",
    "print(\"\\nüèôÔ∏è Analogy: knowledge - human + robotics ‚âà ?\")\n",
    "print(model.wv.most_similar(positive=['knowledge', 'robotics'], negative=['human']))\n",
    "\n",
    "print(\"\\nüìö Most similar to 'Meta':\")\n",
    "# note that we searched for 'meta' in lowercase\n",
    "# to match the tokenization process\n",
    "# otherwise it would not find 'Meta' in the model because this word is not \n",
    "# present in the vocabulary of the trained Word2Vec model. This can happen for several reasons:\n",
    "# During preprocessing, the text is converted to lowercase, and non-alphabetic characters are removed. \n",
    "# As a result, 'Meta' becomes 'meta'.\n",
    "print(model.wv.most_similar('meta')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e3348",
   "metadata": {},
   "source": [
    "### üß† More Use Cases associated with Vector Space Proximity\n",
    "\n",
    "- Long term memory for LLMs.\n",
    "- Semantic Search: based on the meaning or context.\n",
    "- Similarity search for text, images, audio or video data.\n",
    "- Ranking and/or recommendation engine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca919421",
   "metadata": {},
   "source": [
    "#### üß† Vector Proximity in Recommendation Engines\n",
    "\n",
    "Recommendation systems use vector representations of user preferences and item characteristics. In collaborative filtering, each item (e.g., a movie, product, song) is embedded in a vector space based on how users have interacted with it. The proximity (often measured using cosine similarity) between items or between users is used to infer what a user might like next.\n",
    "\n",
    "üí° Key Idea: Items close to those already liked (in vector space) are likely to be good recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f4403",
   "metadata": {},
   "source": [
    "### üß≠ Example ‚Äì Travel Assistant for Bus Schedules\n",
    "\n",
    "You're building a travel assistant for a call center at a travel agency. Customers ask natural language questions like:\n",
    "\n",
    "* \"What time does the next bus to Toronto leave?\"\n",
    "* \"Is there an evening bus to Toronto?\"\n",
    "* \"What time is the first bus to Toronto tomorrow?\"\n",
    "\n",
    "These questions may be phrased differently but carry similar **semantic intent**. We use **vector space proximity** (via word embeddings) to match these queries to predefined responses.\n",
    "\n",
    "#### üîß Other Required Components:\n",
    "\n",
    "* **Predefined Knowledge Base (KB)**: Text entries describing bus schedules (e.g., `\"Next bus to Toronto is at 5:00 PM\"`).\n",
    "* **Embeddings Layer**: Use Word2Vec or Sentence Transformers to represent both queries and KB entries as vectors.\n",
    "* **Similarity Scoring**: Compute cosine similarity to rank possible answers.\n",
    "* **Optional Enhancements**: Named entity recognition (NER), datetime parsing, or LLM for context.\n",
    "\n",
    "---\n",
    "\n",
    "### üî¢ Code: Train Embeddings & Prepare Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f71b8502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Sample knowledge base: schedule answers\n",
    "responses = [\n",
    "    \"The next bus to Toronto leaves at 5:00 PM.\",\n",
    "    \"There is an evening bus to Toronto at 7:30 PM.\",\n",
    "    \"The first bus to Toronto tomorrow departs at 6:15 AM.\",\n",
    "    \"No buses are available after 10:00 PM.\",\n",
    "    \"Toronto-bound buses run every 2 hours.\"\n",
    "]\n",
    "\n",
    "# Preprocessing and training corpus\n",
    "corpus = [resp.lower().replace(\".\", \"\").split() for resp in responses]\n",
    "model = Word2Vec(sentences=corpus, vector_size=50, window=2, min_count=1, sg=1, seed=42)\n",
    "\n",
    "# Get sentence embeddings: average of word vectors\n",
    "def sentence_vector(sentence):\n",
    "    tokens = sentence.lower().replace(\".\", \"\").split()\n",
    "    vecs = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "\n",
    "# Embed all responses\n",
    "response_vectors = np.array([sentence_vector(r) for r in responses])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c551e24",
   "metadata": {},
   "source": [
    "### üß≠ Matching User Queries via Cosine Similarity\n",
    "\n",
    "When a user asks a question like \"What time is the next bus to Toronto?\", we embed the question and compare it against all stored responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "163a2243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: Is there an evening bus to Toronto?\n",
      "\n",
      "Best matches:\n",
      "- (0.74) There is an evening bus to Toronto at 7:30 PM.\n",
      "- (0.34) The first bus to Toronto tomorrow departs at 6:15 AM.\n",
      "- (0.21) The next bus to Toronto leaves at 5:00 PM.\n"
     ]
    }
   ],
   "source": [
    "# User query (can try different phrasings)\n",
    "user_query = \"Is there an evening bus to Toronto?\"\n",
    "\n",
    "# Embed query and compare to stored response vectors\n",
    "query_vec = sentence_vector(user_query)\n",
    "similarities = cosine_similarity([query_vec], response_vectors)[0]\n",
    "\n",
    "# Rank responses\n",
    "ranked_indices = similarities.argsort()[::-1]\n",
    "print(\"User Query:\", user_query)\n",
    "print(\"\\nBest matches:\")\n",
    "for i in ranked_indices[:3]:\n",
    "    print(f\"- ({similarities[i]:.2f}) {responses[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ac191",
   "metadata": {},
   "source": [
    "### üß≠ High-Level System Architecture\n",
    "\n",
    "Here's how vector space proximity fits in a working travel assistant:\n",
    "\n",
    "1. **Input Interface**: Accepts speech or text input from user.\n",
    "2. **Preprocessing Layer**:\n",
    "\n",
    "   * Tokenization\n",
    "   * Lowercasing / Cleaning\n",
    "   * Optional: Named entity recognition (extract city, time)\n",
    "3. **Embedding Layer**:\n",
    "\n",
    "   * Vectorize the user query\n",
    "   * Vectorize all KB entries (if not already done)\n",
    "4. **Similarity Scoring**:\n",
    "\n",
    "   * Compute cosine similarity\n",
    "   * Rank answers based on score\n",
    "5. **Response Selection**:\n",
    "\n",
    "   * Return highest-ranked response\n",
    "   * Optionally add confidence threshold or fallback\n",
    "\n",
    "> üß† This method enables **semantic matching**: It can detect that \"evening\" and \"7:30 PM\" refer to the same idea ‚Äî even though the word \"evening\" is not explicitly in the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f5673a",
   "metadata": {},
   "source": [
    "### üß≠ Extension: Building a Semantic Chatbot for Travel Queries\n",
    "We now extend our bus schedule assistant into a chatbot that:\n",
    "\n",
    "* Accepts user input via a loop or function\n",
    "* Converts questions into vector embeddings\n",
    "* Matches them with high-proximity knowledge base responses\n",
    "* Responds intelligently, even if user phrasing varies\n",
    "\n",
    "This is a rule-based chatbot with semantic matching ‚Äî no LLM involved yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34eb1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_response(user_input, response_texts, response_vecs, model):\n",
    "    def sentence_vector(sentence):\n",
    "        tokens = sentence.lower().replace(\".\", \"\").split()\n",
    "        vecs = [model.wv[word] for word in tokens if word in model.wv]\n",
    "        return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "\n",
    "    user_vec = sentence_vector(user_input)\n",
    "    scores = cosine_similarity([user_vec], response_vecs)[0]\n",
    "    top_index = scores.argmax()\n",
    "    return response_texts[top_index], scores[top_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fce3db",
   "metadata": {},
   "source": [
    "### üß≠ Running the Chatbot in a Loop\n",
    "\n",
    "We now simulate a basic chatbot interface in a terminal-style loop. The user types a query, the assistant finds the best-matching predefined response using cosine similarity.\n",
    "\n",
    "This demonstrates vector space proximity in action in a conversational system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fe14290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöå Travel Assistant Bot (Jupyter Edition)\n",
      "Ask about bus schedules to Toronto. Type 'exit' to quit.\n",
      "\n",
      "Bot: The first bus to Toronto tomorrow departs at 6:15 AM. (similarity: 0.57)\n",
      "\n",
      "Bot: There is an evening bus to Toronto at 7:30 PM. (similarity: 0.60)\n",
      "\n",
      "Bot: The next bus to Toronto leaves at 5:00 PM. (similarity: 0.00)\n",
      "\n",
      "Bot: Safe travels! üëã\n"
     ]
    }
   ],
   "source": [
    "print(\"üöå Travel Assistant Bot (Jupyter Edition)\")\n",
    "print(\"Ask about bus schedules to Toronto. Type 'exit' to quit.\\n\")\n",
    "\n",
    "# Optional: limit to 10 turns for demo purposes\n",
    "max_turns = 10\n",
    "turn = 0\n",
    "\n",
    "while turn < max_turns:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Bot: Safe travels! üëã\")\n",
    "        break\n",
    "\n",
    "    reply, score = get_best_response(user_input, responses, response_vectors, model)\n",
    "    print(f\"Bot: {reply} (similarity: {score:.2f})\\n\")\n",
    "    \n",
    "    turn += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a416b13",
   "metadata": {},
   "source": [
    "### üß≠ Reflection ‚Äì What We Just Built\n",
    "‚úÖ We created a semantic chatbot that understands meaning, not just keywords.\n",
    "\n",
    "üîç Key Concepts Reinforced:\n",
    "\n",
    "* Vector Space Proximity (semantic similarity using cosine distance)\n",
    "* Word Embeddings (Word2Vec)\n",
    "* Response ranking via vector similarity\n",
    "* Stateless rule-based chatbot logic\n",
    "\n",
    "‚öôÔ∏è What‚Äôs Next:\n",
    "\n",
    "* Add NER using spaCy to extract locations and times.\n",
    "* Replace Word2Vec with Sentence Transformers (BERT-style embeddings).\n",
    "* Wrap the chatbot into a web app using Flask or Streamlit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd4cd3",
   "metadata": {},
   "source": [
    "### üß† LangChain + OpenAI for Travel Assistant\n",
    "\n",
    "LangChain is a framework for developing application powered by Large Language Models (LLMs). \n",
    "It was designed and implemented to be:\n",
    "- Data-aware: connecting a language model to other sources of data.\n",
    "- Agentic: allowing a model to interact with its environment.\n",
    "\n",
    "LangChain allows you to build applications using Large Language Models (LLMs) like OpenAI. In this example, we‚Äôre building a simple chatbot that answers bus-related questions using prompt engineering.\n",
    "\n",
    "We'll use:\n",
    "\n",
    "* LangChain to manage prompt flow.\n",
    "* OpenAI as the LLM provider.\n",
    "* A basic retriever for context (optional in simple cases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aacf7c",
   "metadata": {},
   "source": [
    "üì¶ Step 1: Import and Set Up API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d5065b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Check that it's being read correctly\n",
    "print(\"API Key loaded:\", api_key is not None)\n",
    "\n",
    "# üîê Set your OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8682621a",
   "metadata": {},
   "source": [
    "ü§ñ Step 2: Design the Prompt Template\n",
    "\n",
    "Define a system message (behavior) and construct a loop to accept user input. The bot will behave like a travel assistant who only answers bus schedule questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f9b8baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# LLM: Set temperature to 0 for more deterministic outputs\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "system_prompt = SystemMessage(\n",
    "    content=\"You are a helpful travel assistant. Answer only questions about buses from Waterloo to Toronto. Be concise and friendly. If the question is unrelated, politely say you can only answer bus schedule questions.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47181e3",
   "metadata": {},
   "source": [
    "üí¨ Step 3: Run the Chatbot Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1bbb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "try:\n",
    "# üîê Load API key from .env file\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    print(\"Loading OpenAI API key...\", api_key[:8] + \"...\" if api_key else \"None found\")\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    print(\"Loaded OpenAI API key...\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Failed to initialize OpenAI client:\", str(e))\n",
    "\n",
    "# ü§ñ Define system prompt\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful travel assistant. Answer only questions about buses from \"\n",
    "    \"Waterloo to Toronto. Be concise and friendly. If the question is unrelated, \"\n",
    "    \"politely say you can only answer bus schedule questions.\"\n",
    ")\n",
    "\n",
    "# üí¨ Function to send prompt\n",
    "def query_chatgpt(prompt):\n",
    "    try:\n",
    "        print(\"ü§ñ Sending prompt to ChatGPT...\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è Sorry, something went wrong: {str(e)}\"\n",
    "\n",
    "# üí¨ Start chat loop\n",
    "display(Markdown(\"### üöå Travel Assistant Bot\\nAsk about buses from Waterloo to Toronto. Type `exit` to stop.\"))\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Bot: Safe travels! üëã\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"\\nYou asked: {user_input}\")\n",
    "            print(\"\\nü§ñ Bot is thinking...\\n\")\n",
    "            reply = query_chatgpt(user_input)\n",
    "            print(f\"Bot: {reply}\\n\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nBot: Conversation ended manually. Goodbye!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Unexpected error: {str(e)}\\nContinuing...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fdddc9",
   "metadata": {},
   "source": [
    "### üß≠ Reflection ‚Äì Should I Still Use Word2Vec?\n",
    "‚úÖ We've built a chatbot using Word2Vec and vector proximity, but now let's **extend it using LangChain** and **LLMs (e.g., OpenAI)**.\n",
    "\n",
    "With LangChain, we gain access to:\n",
    "- A powerful language model to **understand nuanced questions**\n",
    "- Memory and chaining capabilities for **context-aware conversation**\n",
    "- Seamless integration with APIs, tools, and vector databases\n",
    "\n",
    "üîÅ We'll **still use vector embeddings** for semantic filtering, but **LLMs will generate natural responses**.\n",
    "\n",
    "| Feature         | Word2Vec                         | LLMs (e.g., GPT-4 via LangChain)             |\n",
    "|----------------|----------------------------------|----------------------------------------------|\n",
    "| Speed          | ‚úÖ Very fast                     | ‚ùå Slower due to API latency                 |\n",
    "| Interpretability| ‚úÖ Vectors are analyzable       | ‚ùå Harder to trace how answers are formed    |\n",
    "| Context        | ‚ùå No awareness of conversation  | ‚úÖ Handles multi-turn dialogue well          |\n",
    "| Output Fluency | ‚ùå Simple string matching        | ‚úÖ Fluent and human-like                     |\n",
    "| Cost           | ‚úÖ Free (local)                  | ‚ùå API costs apply (OpenAI)                  |\n",
    "\n",
    "üß© Use both together: Word2Vec to filter or retrieve, LLMs to respond."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
